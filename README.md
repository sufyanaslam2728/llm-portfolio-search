# Project RAG Search API

A FastAPI-based semantic search API using Gemma (via Ollama) with Retrieval-Augmented Generation (RAG). It indexes project data from a JSON file and answers user queries by retrieving and summarizing relevant entries.

---

## ğŸš€ Features

- In-memory semantic search with `sentence-transformers`
- Query-answering with `gemma:2b` via Ollama
- Simple FastAPI endpoint `/search`

---

## ğŸ“ Folder Structure

```
rag-project-search-api/
â”œâ”€â”€ venv/                     â† Virtual environment folder (excluded from Git)
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ llm_rag.py
â”‚   â””â”€â”€ portfolio.json
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation & Setup

---

### 1. Clone the Repo

```bash
git clone https://github.com/sufyanaslam2728/llm-portfolio-search.git
cd llm-portfolio-search
```

---

### 2. Setup Python Environment

```
# Navigate to your project directory
cd rag-project-search-api

# Create virtual environment
python -m venv venv

# Activate it (Linux/macOS)
source venv/bin/activate

# On Windows
venv\Scripts\activate

pip install -r requirements.txt
```

---

### 3. Install Ollama & Pull Model

Install from: https://ollama.com/download

Then pull a model:

```
ollama pull gemma:2b
```

---

### 4. Run the Microservice

```
uvicorn app.main:app --reload

```
